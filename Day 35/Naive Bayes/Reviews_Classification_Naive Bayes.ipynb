{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Hamspam.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expression\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(\"[0-9\" \"]+\",\" \",text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    return text\n",
    "\n",
    "clean = lambda x: clean_text(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.text.apply(clean)\n",
    "data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word frequency\n",
    "freq = pd.Series(' '.join(data['text']).split()).value_counts()[:20] # for top 20\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word frequency after removal of stopwords\n",
    "freq_Sw = pd.Series(' '.join(data['text']).split()).value_counts()[:20] # for top 20\n",
    "freq_Sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectoriser tells the frequency of a word.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "vectorizer = CountVectorizer(min_df = 1, max_df = 0.9)\n",
    "X = vectorizer.fit_transform(data[\"text\"])\n",
    "word_freq_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'occurrences':np.asarray(X.sum(axis=0)).ravel().tolist()})\n",
    "word_freq_df['frequency'] = word_freq_df['occurrences']/np.sum(word_freq_df['occurrences'])\n",
    "#print(word_freq_df.sort('occurrences',ascending = False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF - Term frequency inverse Document Frequencyt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True) #keep top 1000 words\n",
    "doc_vec = vectorizer.fit_transform(data[\"text\"])\n",
    "names_features = vectorizer.get_feature_names()\n",
    "dense = doc_vec.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns = names_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bi-gram\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  #for tri-gram, put ngram_range=(3,3)\n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2_words = get_top_n2_words(data[\"text\"], n=200) #top 200\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
    "top2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bi-gram plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "top20_bigram = top2_df.iloc[0:20,:]\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "plot=sns.barplot(x=top20_bigram[\"Bi-gram\"],y=top20_bigram[\"Freq\"])\n",
    "plot.set_xticklabels(rotation=45,labels = top20_bigram[\"Bi-gram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tri-gram\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_words = get_top_n3_words(data[\"text\"], n=200)\n",
    "top3_df = pd.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tri-gram plot\n",
    "import seaborn as sns\n",
    "top20_trigram = top3_df.iloc[0:20,:]\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "plot=sns.barplot(x=top20_trigram[\"Tri-gram\"],y=top20_trigram[\"Freq\"])\n",
    "plot.set_xticklabels(rotation=45,labels = top20_trigram[\"Tri-gram\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_Total = \" \".join(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud for entire corpus\n",
    "from wordcloud import WordCloud\n",
    "wordcloud_stw = WordCloud(\n",
    "                background_color= 'black',\n",
    "                width = 1800,\n",
    "                height = 1500\n",
    "                ).generate(string_Total)\n",
    "plt.imshow(wordcloud_stw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying naive bayes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_words(i):\n",
    "    return (i.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "email_train,email_test = train_test_split(data,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing email texts into word count matrix format \n",
    "emails_bow = CountVectorizer(analyzer=split_into_words).fit(data.text)\n",
    "\n",
    "# [\"mailing\",\"body\",\"texting\"]\n",
    "# [\"mailing\",\"awesome\",\"good\"]\n",
    "\n",
    "# [\"mailing\",\"body\",\"texting\",\"good\",\"awesome\"]\n",
    "\n",
    "\n",
    "\n",
    "#        \"mailing\" \"body\" \"texting\" \"good\" \"awesome\"\n",
    "#  0          1        1       1        0       0\n",
    " \n",
    "#  1          1        0        0       1       1    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all messages\n",
    "all_emails_matrix = emails_bow.transform(data.text)\n",
    "all_emails_matrix.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training messages\n",
    "train_emails_matrix = emails_bow.transform(email_train.text)\n",
    "train_emails_matrix.shape # (3891,8175)\n",
    "\n",
    "# For testing messages\n",
    "test_emails_matrix = emails_bow.transform(email_test.text)\n",
    "test_emails_matrix.shape # (1668,8175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Without TFIDF matrices ########################\n",
    "# Preparing a naive bayes model on training data set \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB as MB\n",
    "from sklearn.naive_bayes import GaussianNB as GB\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier_mb = MB()\n",
    "classifier_mb.fit(train_emails_matrix,email_train.type)\n",
    "train_pred_m = classifier_mb.predict(train_emails_matrix)\n",
    "accuracy_train_m = np.mean(train_pred_m==email_train.type) # 98%\n",
    "\n",
    "test_pred_m = classifier_mb.predict(test_emails_matrix)\n",
    "accuracy_test_m = np.mean(test_pred_m==email_test.type) # 96%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes \n",
    "classifier_gb = GB()\n",
    "classifier_gb.fit(train_emails_matrix.toarray(),email_train.type.values) # we need to convert tfidf into array format which is compatible for gaussian naive bayes\n",
    "train_pred_g = classifier_gb.predict(train_emails_matrix.toarray())\n",
    "accuracy_train_g = np.mean(train_pred_g==email_train.type) # 95%\n",
    "\n",
    "test_pred_g = classifier_gb.predict(test_emails_matrix.toarray())\n",
    "accuracy_test_g = np.mean(test_pred_g==email_test.type) # 8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Learning Term weighting and normalizing on entire emails\n",
    "tfidf_transformer = TfidfTransformer().fit(all_emails_matrix)\n",
    "\n",
    "# Preparing TFIDF for train emails\n",
    "train_tfidf = tfidf_transformer.transform(train_emails_matrix)\n",
    "\n",
    "train_tfidf.shape # (3891, 6661)\n",
    "\n",
    "# Preparing TFIDF for test emails\n",
    "test_tfidf = tfidf_transformer.transform(test_emails_matrix)\n",
    "\n",
    "test_tfidf.shape #  (1668, 6661)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a naive bayes model on training data set \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB as MB\n",
    "from sklearn.naive_bayes import GaussianNB as GB\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier_mb = MB()\n",
    "classifier_mb.fit(train_tfidf,email_train.type)\n",
    "train_pred_m = classifier_mb.predict(train_tfidf)\n",
    "accuracy_train_m = np.mean(train_pred_m==email_train.type) # 96%\n",
    "\n",
    "test_pred_m = classifier_mb.predict(test_tfidf)\n",
    "accuracy_test_m = np.mean(test_pred_m==email_test.type) # 96%\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes \n",
    "classifier_gb = GB()\n",
    "classifier_gb.fit(train_tfidf.toarray(),email_train.type.values) # we need to convert tfidf into array format which is compatible for gaussian naive bayes\n",
    "train_pred_g = classifier_gb.predict(train_tfidf.toarray())\n",
    "accuracy_train_g = np.mean(train_pred_g==email_train.type) # 95%\n",
    "test_pred_g = classifier_gb.predict(test_tfidf.toarray())\n",
    "accuracy_test_g = np.mean(test_pred_g==email_test.type) # 88%\n",
    "\n",
    "# inplace of tfidf we can also use train_emails_matrix and test_emails_matrix instead of term inverse document frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
